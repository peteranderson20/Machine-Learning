{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b72df611-ccd7-4cfa-a006-386514ba6918",
   "metadata": {},
   "source": [
    "**Notes**\n",
    "- .gz: compressed CSVs with no header, so I will need to provide column names from kddcup.names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5c049874-9957-4cd3-ba4c-2a02c7530312",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'uci_id': 2, 'name': 'Adult', 'repository_url': 'https://archive.ics.uci.edu/dataset/2/adult', 'data_url': 'https://archive.ics.uci.edu/static/public/2/data.csv', 'abstract': 'Predict whether annual income of an individual exceeds $50K/yr based on census data. Also known as \"Census Income\" dataset. ', 'area': 'Social Science', 'tasks': ['Classification'], 'characteristics': ['Multivariate'], 'num_instances': 48842, 'num_features': 14, 'feature_types': ['Categorical', 'Integer'], 'demographics': ['Age', 'Income', 'Education Level', 'Other', 'Race', 'Sex'], 'target_col': ['income'], 'index_col': None, 'has_missing_values': 'yes', 'missing_values_symbol': 'NaN', 'year_of_dataset_creation': 1996, 'last_updated': 'Tue Sep 24 2024', 'dataset_doi': '10.24432/C5XW20', 'creators': ['Barry Becker', 'Ronny Kohavi'], 'intro_paper': None, 'additional_info': {'summary': \"Extraction was done by Barry Becker from the 1994 Census database.  A set of reasonably clean records was extracted using the following conditions: ((AAGE>16) && (AGI>100) && (AFNLWGT>1)&& (HRSWK>0))\\n\\nPrediction task is to determine whether a person's income is over $50,000 a year.\\n\", 'purpose': None, 'funded_by': None, 'instances_represent': None, 'recommended_data_splits': None, 'sensitive_data': None, 'preprocessing_description': None, 'variable_info': 'Listing of attributes:\\r\\n\\r\\n>50K, <=50K.\\r\\n\\r\\nage: continuous.\\r\\nworkclass: Private, Self-emp-not-inc, Self-emp-inc, Federal-gov, Local-gov, State-gov, Without-pay, Never-worked.\\r\\nfnlwgt: continuous.\\r\\neducation: Bachelors, Some-college, 11th, HS-grad, Prof-school, Assoc-acdm, Assoc-voc, 9th, 7th-8th, 12th, Masters, 1st-4th, 10th, Doctorate, 5th-6th, Preschool.\\r\\neducation-num: continuous.\\r\\nmarital-status: Married-civ-spouse, Divorced, Never-married, Separated, Widowed, Married-spouse-absent, Married-AF-spouse.\\r\\noccupation: Tech-support, Craft-repair, Other-service, Sales, Exec-managerial, Prof-specialty, Handlers-cleaners, Machine-op-inspct, Adm-clerical, Farming-fishing, Transport-moving, Priv-house-serv, Protective-serv, Armed-Forces.\\r\\nrelationship: Wife, Own-child, Husband, Not-in-family, Other-relative, Unmarried.\\r\\nrace: White, Asian-Pac-Islander, Amer-Indian-Eskimo, Other, Black.\\r\\nsex: Female, Male.\\r\\ncapital-gain: continuous.\\r\\ncapital-loss: continuous.\\r\\nhours-per-week: continuous.\\r\\nnative-country: United-States, Cambodia, England, Puerto-Rico, Canada, Germany, Outlying-US(Guam-USVI-etc), India, Japan, Greece, South, China, Cuba, Iran, Honduras, Philippines, Italy, Poland, Jamaica, Vietnam, Mexico, Portugal, Ireland, France, Dominican-Republic, Laos, Ecuador, Taiwan, Haiti, Columbia, Hungary, Guatemala, Nicaragua, Scotland, Thailand, Yugoslavia, El-Salvador, Trinadad&Tobago, Peru, Hong, Holand-Netherlands.', 'citation': None}}\n",
      "              name     role         type      demographic  \\\n",
      "0              age  Feature      Integer              Age   \n",
      "1        workclass  Feature  Categorical           Income   \n",
      "2           fnlwgt  Feature      Integer             None   \n",
      "3        education  Feature  Categorical  Education Level   \n",
      "4    education-num  Feature      Integer  Education Level   \n",
      "5   marital-status  Feature  Categorical            Other   \n",
      "6       occupation  Feature  Categorical            Other   \n",
      "7     relationship  Feature  Categorical            Other   \n",
      "8             race  Feature  Categorical             Race   \n",
      "9              sex  Feature       Binary              Sex   \n",
      "10    capital-gain  Feature      Integer             None   \n",
      "11    capital-loss  Feature      Integer             None   \n",
      "12  hours-per-week  Feature      Integer             None   \n",
      "13  native-country  Feature  Categorical            Other   \n",
      "14          income   Target       Binary           Income   \n",
      "\n",
      "                                          description units missing_values  \n",
      "0                                                 N/A  None             no  \n",
      "1   Private, Self-emp-not-inc, Self-emp-inc, Feder...  None            yes  \n",
      "2                                                None  None             no  \n",
      "3    Bachelors, Some-college, 11th, HS-grad, Prof-...  None             no  \n",
      "4                                                None  None             no  \n",
      "5   Married-civ-spouse, Divorced, Never-married, S...  None             no  \n",
      "6   Tech-support, Craft-repair, Other-service, Sal...  None            yes  \n",
      "7   Wife, Own-child, Husband, Not-in-family, Other...  None             no  \n",
      "8   White, Asian-Pac-Islander, Amer-Indian-Eskimo,...  None             no  \n",
      "9                                       Female, Male.  None             no  \n",
      "10                                               None  None             no  \n",
      "11                                               None  None             no  \n",
      "12                                               None  None             no  \n",
      "13  United-States, Cambodia, England, Puerto-Rico,...  None            yes  \n",
      "14                                       >50K, <=50K.  None             no  \n"
     ]
    }
   ],
   "source": [
    "from ucimlrepo import fetch_ucirepo \n",
    "  \n",
    "# fetch dataset \n",
    "adult = fetch_ucirepo(id=2) \n",
    "  \n",
    "# data (as pandas dataframes) \n",
    "X = adult.data.features \n",
    "y = adult.data.targets \n",
    "  \n",
    "# metadata \n",
    "print(adult.metadata) \n",
    "  \n",
    "# variable information \n",
    "print(adult.variables) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "414f9482-3d0e-4875-80bc-745d73098547",
   "metadata": {},
   "source": [
    "# Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ec288416-405d-4912-a074-00d16c38653e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>fnlwgt</th>\n",
       "      <th>education-num</th>\n",
       "      <th>capital-gain</th>\n",
       "      <th>capital-loss</th>\n",
       "      <th>hours-per-week</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>48842.000000</td>\n",
       "      <td>4.884200e+04</td>\n",
       "      <td>48842.000000</td>\n",
       "      <td>48842.000000</td>\n",
       "      <td>48842.000000</td>\n",
       "      <td>48842.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>38.643585</td>\n",
       "      <td>1.896641e+05</td>\n",
       "      <td>10.078089</td>\n",
       "      <td>1079.067626</td>\n",
       "      <td>87.502314</td>\n",
       "      <td>40.422382</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>13.710510</td>\n",
       "      <td>1.056040e+05</td>\n",
       "      <td>2.570973</td>\n",
       "      <td>7452.019058</td>\n",
       "      <td>403.004552</td>\n",
       "      <td>12.391444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>17.000000</td>\n",
       "      <td>1.228500e+04</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>28.000000</td>\n",
       "      <td>1.175505e+05</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>40.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>37.000000</td>\n",
       "      <td>1.781445e+05</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>40.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>48.000000</td>\n",
       "      <td>2.376420e+05</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>45.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>90.000000</td>\n",
       "      <td>1.490400e+06</td>\n",
       "      <td>16.000000</td>\n",
       "      <td>99999.000000</td>\n",
       "      <td>4356.000000</td>\n",
       "      <td>99.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                age        fnlwgt  education-num  capital-gain  capital-loss  \\\n",
       "count  48842.000000  4.884200e+04   48842.000000  48842.000000  48842.000000   \n",
       "mean      38.643585  1.896641e+05      10.078089   1079.067626     87.502314   \n",
       "std       13.710510  1.056040e+05       2.570973   7452.019058    403.004552   \n",
       "min       17.000000  1.228500e+04       1.000000      0.000000      0.000000   \n",
       "25%       28.000000  1.175505e+05       9.000000      0.000000      0.000000   \n",
       "50%       37.000000  1.781445e+05      10.000000      0.000000      0.000000   \n",
       "75%       48.000000  2.376420e+05      12.000000      0.000000      0.000000   \n",
       "max       90.000000  1.490400e+06      16.000000  99999.000000   4356.000000   \n",
       "\n",
       "       hours-per-week  \n",
       "count    48842.000000  \n",
       "mean        40.422382  \n",
       "std         12.391444  \n",
       "min          1.000000  \n",
       "25%         40.000000  \n",
       "50%         40.000000  \n",
       "75%         45.000000  \n",
       "max         99.000000  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "96549064-9423-4718-83be-10e1019122dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 48842 entries, 0 to 48841\n",
      "Data columns (total 14 columns):\n",
      " #   Column          Non-Null Count  Dtype \n",
      "---  ------          --------------  ----- \n",
      " 0   age             48842 non-null  int64 \n",
      " 1   workclass       47879 non-null  object\n",
      " 2   fnlwgt          48842 non-null  int64 \n",
      " 3   education       48842 non-null  object\n",
      " 4   education-num   48842 non-null  int64 \n",
      " 5   marital-status  48842 non-null  object\n",
      " 6   occupation      47876 non-null  object\n",
      " 7   relationship    48842 non-null  object\n",
      " 8   race            48842 non-null  object\n",
      " 9   sex             48842 non-null  object\n",
      " 10  capital-gain    48842 non-null  int64 \n",
      " 11  capital-loss    48842 non-null  int64 \n",
      " 12  hours-per-week  48842 non-null  int64 \n",
      " 13  native-country  48568 non-null  object\n",
      "dtypes: int64(6), object(8)\n",
      "memory usage: 5.2+ MB\n"
     ]
    }
   ],
   "source": [
    "X.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5f1415f1-3eac-42ca-a2f8-4a1a13e71ade",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 48842 entries, 0 to 48841\n",
      "Data columns (total 1 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   income  48842 non-null  object\n",
      "dtypes: object(1)\n",
      "memory usage: 381.7+ KB\n"
     ]
    }
   ],
   "source": [
    "y.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9427ed0e-1ece-4496-9c5c-b5bf2412dc3a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>income</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>48842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>24720</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       income\n",
       "count   48842\n",
       "unique      4\n",
       "top     <=50K\n",
       "freq    24720"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "30ebb967-7ba4-476c-93cf-cbc1386996a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['<=50K', '>50K', '<=50K.', '>50K.'], dtype=object)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y['income'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7fb340e-6c28-4f22-9cf8-4c9730dce15d",
   "metadata": {},
   "source": [
    "'>50K' and '>50K.' is 1, '<=50K' and '<=50K.' is 0. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "10a15ae1-8de7-4bb2-89de-024400664954",
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, value in y['income'].items():\n",
    "    if value in ['<=50K', '<=50K.']:\n",
    "        y.at[idx, 'income'] = 0\n",
    "    else:\n",
    "        y.at[idx, 'income'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5c57386c-1c71-4606-a734-dd41a5ccced0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1], dtype=object)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y['income'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "70231042-97b2-432b-ab01-2b500aef8def",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/83/qx33zxpd4cjb39h49x9vvynr0000gn/T/ipykernel_2712/3755289315.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  y['income'] = y['income'].astype(int)\n"
     ]
    }
   ],
   "source": [
    "y['income'] = y['income'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e896636f-8b21-407c-b243-106de2352e95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y['income'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3d48fcf2-6655-4d99-bbd6-a90b1ea86089",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>income</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>48842.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.239282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.426649</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             income\n",
       "count  48842.000000\n",
       "mean       0.239282\n",
       "std        0.426649\n",
       "min        0.000000\n",
       "25%        0.000000\n",
       "50%        0.000000\n",
       "75%        0.000000\n",
       "max        1.000000"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "448668a7-7fb9-46f4-a792-a3e0c92a28e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 48842 entries, 0 to 48841\n",
      "Data columns (total 1 columns):\n",
      " #   Column  Non-Null Count  Dtype\n",
      "---  ------  --------------  -----\n",
      " 0   income  48842 non-null  int64\n",
      "dtypes: int64(1)\n",
      "memory usage: 381.7 KB\n"
     ]
    }
   ],
   "source": [
    "y.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "01d27e90-042b-4c51-9625-42e9cd4261c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 48842 entries, 0 to 48841\n",
      "Data columns (total 14 columns):\n",
      " #   Column          Non-Null Count  Dtype \n",
      "---  ------          --------------  ----- \n",
      " 0   age             48842 non-null  int64 \n",
      " 1   workclass       47879 non-null  object\n",
      " 2   fnlwgt          48842 non-null  int64 \n",
      " 3   education       48842 non-null  object\n",
      " 4   education-num   48842 non-null  int64 \n",
      " 5   marital-status  48842 non-null  object\n",
      " 6   occupation      47876 non-null  object\n",
      " 7   relationship    48842 non-null  object\n",
      " 8   race            48842 non-null  object\n",
      " 9   sex             48842 non-null  object\n",
      " 10  capital-gain    48842 non-null  int64 \n",
      " 11  capital-loss    48842 non-null  int64 \n",
      " 12  hours-per-week  48842 non-null  int64 \n",
      " 13  native-country  48568 non-null  object\n",
      "dtypes: int64(6), object(8)\n",
      "memory usage: 5.2+ MB\n"
     ]
    }
   ],
   "source": [
    "X.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c62ea5de-e6bf-46ae-859e-b1bb09642f89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1221\n"
     ]
    }
   ],
   "source": [
    "num_rows_with_null = X.isnull().any(axis=1).sum()\n",
    "print(num_rows_with_null)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6aad4647-9968-42fc-9494-2f9219cf7d76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.int64(4262)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(X == '?').sum().sum() #sum per column, total across all columns "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "364db5d3-3cb9-4648-aec1-80c2ec746c2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3620\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "X = X.replace('?', pd.NA)\n",
    "\n",
    "num_rows_with_null = X.isna().any(axis=1).sum()\n",
    "print(num_rows_with_null)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "32c6c72e-bd0e-46a6-9551-8089d7d85cf8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['State-gov', 'Self-emp-not-inc', 'Private', 'Federal-gov',\n",
       "       'Local-gov', <NA>, 'Self-emp-inc', 'Without-pay', 'Never-worked',\n",
       "       nan], dtype=object)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X['workclass'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3531f331-9e61-44ab-8eda-b905e0a792d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dropping rows\n",
    "\n",
    "mask = ~X.isna().any(axis=1)\n",
    "\n",
    "#X.isna(): creates a dataframe of true/false values (true where a cell in X is missing, false otherwise)\n",
    "#.any(axis=1): checks across each row (axis = 1 means across columns) \n",
    "# ~ is logical not in pandas \n",
    "\n",
    "\n",
    "X = X.loc[mask].copy()\n",
    "y = y.loc[mask].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "491c82f4-f4a7-4aeb-8d0e-f0b16804e9c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 45222 entries, 0 to 48841\n",
      "Data columns (total 14 columns):\n",
      " #   Column          Non-Null Count  Dtype \n",
      "---  ------          --------------  ----- \n",
      " 0   age             45222 non-null  int64 \n",
      " 1   workclass       45222 non-null  object\n",
      " 2   fnlwgt          45222 non-null  int64 \n",
      " 3   education       45222 non-null  object\n",
      " 4   education-num   45222 non-null  int64 \n",
      " 5   marital-status  45222 non-null  object\n",
      " 6   occupation      45222 non-null  object\n",
      " 7   relationship    45222 non-null  object\n",
      " 8   race            45222 non-null  object\n",
      " 9   sex             45222 non-null  object\n",
      " 10  capital-gain    45222 non-null  int64 \n",
      " 11  capital-loss    45222 non-null  int64 \n",
      " 12  hours-per-week  45222 non-null  int64 \n",
      " 13  native-country  45222 non-null  object\n",
      "dtypes: int64(6), object(8)\n",
      "memory usage: 5.2+ MB\n"
     ]
    }
   ],
   "source": [
    "X.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a2e83fb6-bd71-487c-8e45-44b538c4b91c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "workclass: 7\n",
      "education: 16\n",
      "marital-status: 7\n",
      "occupation: 14\n",
      "relationship: 6\n",
      "race: 5\n",
      "sex: 2\n",
      "native-country: 41\n"
     ]
    }
   ],
   "source": [
    "for column in X.columns:\n",
    "    if X[column].dtype == 'object':\n",
    "        print(f\"{column}: {len(X[column].unique())}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbb27534-5ec8-4fb7-9086-0b2448c637e9",
   "metadata": {},
   "source": [
    "## Options for Encoding Categorical Variables\n",
    "1. One-Hot Encoding\n",
    "   - Pros: no information loss\n",
    "   - Cons: expldoes feature size, leads to sparse data\n",
    "2. Label Encoding\n",
    "   - How it works: Assign each category an integer label\n",
    "   - Pros: keeps dimensionality low\n",
    "   - Cons: NN may interpret numbers as ordinal when in reality they have no mathematical relationship\n",
    "3. Target Encoding / Mean Encoding\n",
    "   - How it works: Replaces each category with a numerical statistic, which could be the most common category\n",
    "   - Pros: compact\n",
    "   - Cons: loss of information\n",
    "4. Embedding Layers\n",
    "   - How it works: each category is mapped to a dense vector of learned weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6973890-97f8-4603-b74c-7f51fb59d9bd",
   "metadata": {},
   "source": [
    "# Preprocessing\n",
    "- We already handled missing data, and categorical data will be handled when we setup the model since there will be an embedding layer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b213d632-dc48-4eb8-adc8-8d467ada8093",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "cat_cols = [c for c in X.columns if X[c].dtype == 'object']\n",
    "num_cols = X.select_dtypes(include=[np.number]).columns\n",
    "\n",
    "X_train_full, X_test, y_train_full, y_test = train_test_split(\n",
    "    X, y,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=y\n",
    ")\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_train_full, y_train_full,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=y_train_full\n",
    ")\n",
    "\n",
    "\n",
    "#stratification to y to make sure distribution of classes is the same in train and validaiton. \n",
    "\n",
    "\n",
    "\n",
    "                    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ec567e5-11e6-4179-8f72-c5a3342b983e",
   "metadata": {},
   "source": [
    "### Converting categorical columns to integer IDS\n",
    "\n",
    "Embedding layers expect integer indices, so the categories must be mapped to numbers with 0 reserved for UNK (unknown/unseen categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "13116afc-2c85-4b13-b8d3-fa063bc31427",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'workclass': {'unk': 0, 'Private': 1, 'State-gov': 2, 'Self-emp-not-inc': 3, 'Federal-gov': 4, 'Local-gov': 5, 'Self-emp-inc': 6, 'Without-pay': 7}, 'education': {'unk': 0, 'Bachelors': 1, 'HS-grad': 2, 'Masters': 3, 'Some-college': 4, '7th-8th': 5, 'Prof-school': 6, '11th': 7, '10th': 8, 'Assoc-voc': 9, '5th-6th': 10, '12th': 11, 'Assoc-acdm': 12, '9th': 13, 'Doctorate': 14, '1st-4th': 15, 'Preschool': 16}, 'marital-status': {'unk': 0, 'Married-civ-spouse': 1, 'Never-married': 2, 'Divorced': 3, 'Separated': 4, 'Widowed': 5, 'Married-spouse-absent': 6, 'Married-AF-spouse': 7}, 'occupation': {'unk': 0, 'Exec-managerial': 1, 'Adm-clerical': 2, 'Prof-specialty': 3, 'Sales': 4, 'Farming-fishing': 5, 'Machine-op-inspct': 6, 'Transport-moving': 7, 'Craft-repair': 8, 'Tech-support': 9, 'Other-service': 10, 'Protective-serv': 11, 'Priv-house-serv': 12, 'Handlers-cleaners': 13, 'Armed-Forces': 14}, 'relationship': {'unk': 0, 'Husband': 1, 'Unmarried': 2, 'Own-child': 3, 'Other-relative': 4, 'Wife': 5, 'Not-in-family': 6}, 'race': {'unk': 0, 'White': 1, 'Black': 2, 'Other': 3, 'Asian-Pac-Islander': 4, 'Amer-Indian-Eskimo': 5}, 'sex': {'unk': 0, 'Male': 1, 'Female': 2}, 'native-country': {'unk': 0, 'United-States': 1, 'Cuba': 2, 'Dominican-Republic': 3, 'India': 4, 'Italy': 5, 'Canada': 6, 'China': 7, 'Guatemala': 8, 'Japan': 9, 'Ecuador': 10, 'Vietnam': 11, 'Mexico': 12, 'Philippines': 13, 'Puerto-Rico': 14, 'England': 15, 'Jamaica': 16, 'El-Salvador': 17, 'South': 18, 'Poland': 19, 'Taiwan': 20, 'Germany': 21, 'Portugal': 22, 'Iran': 23, 'Peru': 24, 'Trinadad&Tobago': 25, 'Nicaragua': 26, 'France': 27, 'Haiti': 28, 'Ireland': 29, 'Columbia': 30, 'Greece': 31, 'Yugoslavia': 32, 'Outlying-US(Guam-USVI-etc)': 33, 'Thailand': 34, 'Scotland': 35, 'Cambodia': 36, 'Hong': 37, 'Hungary': 38, 'Laos': 39, 'Honduras': 40, 'Holand-Netherlands': 41}}\n"
     ]
    }
   ],
   "source": [
    "#first we build the mappings from the training dataset only\n",
    "cat_maps = {}\n",
    "\n",
    "for c in cat_cols:\n",
    "    categories = X_train[c].unique()\n",
    "    mapping = {\"unk\": 0}\n",
    "    for index, value in enumerate(categories, start=1):\n",
    "        mapping[value] = index\n",
    "    cat_maps[c] = mapping\n",
    "\n",
    "print(cat_maps)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1bd56542-0f0c-4610-acb7-10afcf9597b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train:\n",
      "        workclass  education  marital-status  occupation  relationship  race  \\\n",
      "5479           1          1               1           1             1     1   \n",
      "11287          2          2               2           2             2     2   \n",
      "31907          1          3               1           3             1     1   \n",
      "9695           3          4               1           4             1     1   \n",
      "43382          1          4               2           2             3     1   \n",
      "...          ...        ...             ...         ...           ...   ...   \n",
      "13028          1          2               3           7             6     1   \n",
      "38378          1          2               1           2             1     1   \n",
      "30315          1          2               1           4             5     1   \n",
      "11081          1          4               1           7             1     1   \n",
      "23786          1          4               2          13             3     2   \n",
      "\n",
      "       sex  native-country  \n",
      "5479     1               1  \n",
      "11287    2               1  \n",
      "31907    1               2  \n",
      "9695     1               1  \n",
      "43382    2               1  \n",
      "...    ...             ...  \n",
      "13028    1               1  \n",
      "38378    1               1  \n",
      "30315    2               1  \n",
      "11081    1               1  \n",
      "23786    2               1  \n",
      "\n",
      "[28941 rows x 8 columns]\n",
      "\n",
      "Val:\n",
      "        workclass  education  marital-status  occupation  relationship  race  \\\n",
      "15244          1          4               1           4             1     1   \n",
      "26994          1          2               1           6             1     1   \n",
      "23561          1          2               4           2             2     1   \n",
      "17681          1          4               2           4             6     1   \n",
      "45343          1          4               2           7             6     1   \n",
      "...          ...        ...             ...         ...           ...   ...   \n",
      "46688          1          5               2          13             3     1   \n",
      "8863           1          2               4           8             6     1   \n",
      "12138          5          2               2          10             2     1   \n",
      "14639          1          1               2           1             6     1   \n",
      "28779          1          8               2           4             6     1   \n",
      "\n",
      "       sex  native-country  \n",
      "15244    1               1  \n",
      "26994    1               1  \n",
      "23561    2               1  \n",
      "17681    1               1  \n",
      "45343    1               1  \n",
      "...    ...             ...  \n",
      "46688    1               1  \n",
      "8863     1               1  \n",
      "12138    2               1  \n",
      "14639    1               1  \n",
      "28779    2               1  \n",
      "\n",
      "[7236 rows x 8 columns]\n"
     ]
    }
   ],
   "source": [
    "def map_categories(column, mapping):\n",
    "    return column.map(mapping).fillna(0).astype(\"int64\")\n",
    "\n",
    "# I removed missing columns from X already, but here\n",
    "# map(mapping) will return NaN if it encounters a category not seen in cat_maps\n",
    "# that is, not seen in training but seen in the validation set\n",
    "# so, I put fillna(0) to put those unseen cateogires into the unk bucket\n",
    "\n",
    "\n",
    "#the map_categories function only works on one column at a time,\n",
    "#so I put a lambda function so it applies map_categories to all columns in cat_cols\n",
    "\n",
    "train_categories = X_train[cat_cols].apply(lambda column: map_categories(column, cat_maps[column.name]))\n",
    "val_categories = X_val[cat_cols].apply(lambda column: map_categories(column, cat_maps[column.name]))\n",
    "\n",
    "print(\"Train:\\n\", train_categories)\n",
    "print(\"\\nVal:\\n\", val_categories)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17a58cce-2e27-49a0-8bbd-ce969cd5546e",
   "metadata": {},
   "source": [
    "### Scaling the numeric columns "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ed9fe5d2-25b1-4e14-9e7b-c2263fe179bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "#standard scaler transforms integers into real numbers\n",
    "#PyTorch layers expect inputs of type torch.float32\n",
    "#It is float32 instead of float64 to use less memory, therefore the training process is faster\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train[num_cols]).astype(\"float32\")\n",
    "X_val_scaled = scaler.transform(X_val[num_cols]).astype(\"float32\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd572e2e-511c-48ff-a5fe-53da63975c9c",
   "metadata": {},
   "source": [
    "### Building categorical ID matrices\n",
    "This step is about converting the categorical columns into the shape that that the embedding layers expect, which is (batch_size, num_cat_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "5c391a1b-fb33-4783-aa0f-275ea5b92ab9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(28941, 8)\n",
      "[[1 1 1 1 1 1 1 1]\n",
      " [2 2 2 2 2 2 2 1]\n",
      " [1 3 1 3 1 1 1 2]]\n"
     ]
    }
   ],
   "source": [
    "X_train_categories = train_categories.to_numpy(dtype=\"int64\")\n",
    "X_val_categories = val_categories.to_numpy(dtype=\"int64\")\n",
    "\n",
    "print(X_train_categories.shape) #(num_rows, num_cat_cols)\n",
    "print(X_train_categories[:3]) #each row = one training example\n",
    "                              #each column = one encoded ID for a categorical feature"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2f779b2-8716-4a03-ac43-eec2a274f30c",
   "metadata": {},
   "source": [
    "### Make y float32\n",
    "- We will be using nn.BCEWithLogitsLoss\n",
    "- The model will output a float32 logit\n",
    "- The loss compares that float32 logit against the target tensor (y), and if y is int, PyTorch will throw a dtype mismatch error "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "502a8bdd-6680-42cc-a0d3-ef0f2da5c2d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n"
     ]
    }
   ],
   "source": [
    "print(type(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "76177e1d-09ca-4c34-9708-b57c48501230",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_array = y_train.astype(\"float32\").to_numpy()\n",
    "y_val_array = y_val.astype(\"float32\").to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "3cd751c2-46be-4fa0-b346-ac4cb9c50c21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "print(type(y_train_array))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cbac700-eaeb-4bc0-bf5a-b23710771e86",
   "metadata": {},
   "source": [
    "### The Embedding Layer\n",
    "- Mathematically: a lookup table (a matrix of learnable weights)\n",
    "- Shape: (num_categories, embedding_dim); the rows are categories (0, 1, 2, etc.) and the columns are the latent features for each category. So, each categorical column will get its own embedding layer.\n",
    "- It expects a tensor of integer IDs (torch.int64 aka LongTensor) of shape (batch_size, num_cat_cols). Each row is a sample, each column is the category (integer IDs)\n",
    "- For each categorical column it outputs an embedding vector as float32. For multiple categorical columns, their embeddings will be contatenated.\n",
    "- The embedding vectors themselves are learned during training\n",
    "\n",
    "\n",
    "<br>\n",
    "In short: column --> embedding vector \n",
    "<br>\n",
    "The next step will be figuring out how many categories each column has, then picking embedding dimensions for each. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c453adac-fd50-4708-a1ee-09c8d3313fbd",
   "metadata": {},
   "source": [
    "### Vocab sizes \n",
    "The cleanest, stable vocab size is just the mapping length, or the toal number of unique integer IDs that can appear in a given column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "ba0308d1-0351-490a-9e2f-27e5ac0c312a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab_sizes [8, 17, 8, 15, 7, 6, 3, 42]\n",
      "emb_dims [(8, 13), (17, 16), (8, 13), (15, 16), (7, 13), (6, 13), (3, 11), (42, 20)]\n"
     ]
    }
   ],
   "source": [
    "vocab_sizes = [len(cat_maps[c]) for c in cat_cols]\n",
    "\n",
    "#next, we have a function that computes the embedding dimension\n",
    "#in a way that balances model expressiveness with efficiency\n",
    "\n",
    "def pick_emb_dim(vocab_size):\n",
    "    return min(50, max(4, int(round(vocab_size**0.25 * 8))))\n",
    "\n",
    "#Next we compute the embedding dimensions for each column\n",
    "emb_dims = [(vocab_size, pick_emb_dim(vocab_size)) for vocab_size in vocab_sizes]\n",
    "\n",
    "print(\"vocab_sizes\", vocab_sizes)\n",
    "print(\"emb_dims\", emb_dims)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "522db6a1-cb8e-4922-92e4-261666cd1e17",
   "metadata": {},
   "source": [
    "# Modeling\n",
    "This part of the project is where the PyTorch philosphy shines.\n",
    "- Nothing is hidden, and I define how data is stored, accessed, batched, and shuffled.\n",
    "\n",
    "For this, PyTorch gives two pieces:\n",
    "1. Dataset: how to get a single sample, and we can neatly package each batch as (x_cats, x_nums, y)\n",
    "2. DataLoader: how to turn that Dataset into batches with shuffling, batching, multiprocessing, moving to GPU, etc. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b2c5c0d-51b4-42a9-a5fa-c0479904b92c",
   "metadata": {},
   "source": [
    "## Dataset and DataLoader "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "17c899ae-ed3f-407b-9578-434470434bf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "\n",
    "class TabularDS(Dataset):\n",
    "    def __init__(self, X_cats, X_nums, y):\n",
    "        self.X_cats = X_cats\n",
    "        self.X_nums = X_nums\n",
    "        self.y = y.reshape(-1, 1) #forces column vector (N,1)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.y.shape[0]\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        return (\n",
    "            torch.tensor(self.X_cats[i], dtype=torch.long),\n",
    "            torch.tensor(self.X_nums[i], dtype=torch.float32),\n",
    "            torch.tensor(self.y[i], dtype=torch.float32),\n",
    "        )\n",
    "train_dataset = TabularDS(X_train_categories, X_train_scaled, y_train_array)\n",
    "val_dataset = TabularDS(X_val_categories, X_val_scaled, y_val_array)\n",
    "\n",
    "#smaller batch size for training batches helps the optimizer see more gradient noise,\n",
    "#improving generalization\n",
    "#in validation, we aren't updating any weights so higher batch size means faster evaluation\n",
    "#in addition, we shuffle the training set so the model doesn't overfit to the order in the data\n",
    "#we set shuffling to false for validation loader as it provides no benefit \n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=512, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=1024, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db29453c-ae44-4d8a-ab9d-c158fbefde00",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "13aeeda2-fbf9-45ff-9478-73f12d440de2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class CatEmbMLP(nn.Module):\n",
    "    #hidden=(128, 64) means first linear layer is 128 units and second linear layer is 64\n",
    "    def __init__(self, emb_dims, n_num, hidden=(256, 128, 64), p=0.01):\n",
    "        super().__init__()\n",
    "        #for each categorical column, build one embedding table\n",
    "        #v = vocab size (number of categories)\n",
    "        #d = embedding dimension (chosen earlier)\n",
    "        self.embs = nn.ModuleList([nn.Embedding(v, d) for (v, d) in emb_dims])\n",
    "        self.emb_drop = nn.Dropout(p)\n",
    "\n",
    "        #emb_dims is of shape (vocab_sizei, emb_dimi)\n",
    "        #the next line refers to the total embedding output size\n",
    "        #n_num is the amount of numerical columns\n",
    "        in_dim = sum(d for _, d in emb_dims) + n_num\n",
    "        layers = []\n",
    "        for h in hidden:\n",
    "            layers += [nn.Linear(in_dim, h), nn.ReLU(), nn.Dropout(p)]\n",
    "            in_dim = h #reset the input dimensions \n",
    "        layers += [nn.Linear(in_dim, 1)] #binary logit\n",
    "        self.mlp = nn.Sequential(*layers)\n",
    "\n",
    "    #x_cat: categorical input tensor\n",
    "        #shape: (batch_size, n_cat_cols)\n",
    "        #dtype: torch.int64 (LongTensor)\n",
    "    #x_num: numeric input tensor\n",
    "        #shape: (batch_size, n_num_cols)\n",
    "        #dtype: torch.float32\n",
    "    def forward(self, x_cat, x_num):\n",
    "        #x_cat[:, i] = all IDs for column i in the batch --> shape (batch_size,)\n",
    "        #emb(x_cat[:, i]) = look up each IDs embedding vector --> shape (batch_size, emb_dim_i)\n",
    "        #emb_list collects these embeddings in a list \n",
    "        emb_list = [emb(x_cat[:, i]) for i, emb in enumerate(self.embs)]\n",
    "        #join the embedding vectors side by side --> (batch_size, sum_of_all_emb_dims)\n",
    "        x = torch.cat(emb_list, dim=1)\n",
    "        #applying dropout for regularization, preventing overfitting to rare cetegories\n",
    "        x = self.emb_drop(x)\n",
    "        #join with numeric features, shape (batch_size, sum_emb_dims + n_num) as defined in __init__\n",
    "        x = torch.cat([x, x_num], dim=1)\n",
    "        #forward pass through the mlp\n",
    "        return self.mlp(x)\n",
    "\n",
    "#seeing if we can take advantage of Apple Silicon GPU \n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "n_num = X_train_scaled.shape[1]\n",
    "model = CatEmbMLP(emb_dims, n_num, hidden=(128, 64), p=0.1).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2b2222d-6eef-41a8-896a-1248eda8ea7a",
   "metadata": {},
   "source": [
    "## Training Loop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ca36fbf-16da-4eb5-89e2-4d9d9aadc988",
   "metadata": {},
   "source": [
    "### Improvements. \n",
    "\n",
    "**Run 1**\n",
    "Epoch 001 | train 0.3054 | val 0.3064 | acc 0.861 | auc 0.916 | lr 2.0e-04\n",
    "Epoch 002 | train 0.3034 | val 0.3067 | acc 0.861 | auc 0.916 | lr 2.0e-04\n",
    "Epoch 003 | train 0.3048 | val 0.3070 | acc 0.860 | auc 0.915 | lr 2.0e-04\n",
    "Epoch 004 | train 0.3039 | val 0.3071 | acc 0.860 | auc 0.915 | lr 2.0e-04\n",
    "Epoch 005 | train 0.3039 | val 0.3073 | acc 0.860 | auc 0.915 | lr 1.0e-04\n",
    "Epoch 006 | train 0.3034 | val 0.3073 | acc 0.860 | auc 0.915 | lr 1.0e-04\n",
    "Epoch 007 | train 0.3038 | val 0.3073 | acc 0.860 | auc 0.915 | lr 1.0e-04\n",
    "Epoch 008 | train 0.3027 | val 0.3074 | acc 0.859 | auc 0.915 | lr 1.0e-04\n",
    "Epoch 009 | train 0.3022 | val 0.3074 | acc 0.860 | auc 0.915 | lr 5.0e-05\n",
    "Epoch 010 | train 0.3024 | val 0.3074 | acc 0.860 | auc 0.915 | lr 5.0e-05\n",
    "Epoch 011 | train 0.3028 | val 0.3075 | acc 0.859 | auc 0.915 | lr 5.0e-05\n",
    "Early stopping at epoch 11 (best val loss: 0.3064)\n",
    "Loaded best model (val loss = 0.3064 )\n",
    "\n",
    "- it seems that train accuracy ~= val accuracy so it seems like the model isn't learning that much.\n",
    "- We could try to make the model deeper/wider, try a different LR-scheduler (warmup + cosine, or OneCycleLR, add learned normalization, increase embedding dims by 25-50%, increase patience after I try capacity / LR changes.\n",
    "- To compare, we could run XGBoost. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "2a848191-13de-47cc-ba9d-187ca41134ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 001 | train 0.3077 | val 0.3071 | acc 0.859 | auc 0.916 | lr 2.0e-04\n",
      "Epoch 002 | train 0.3056 | val 0.3074 | acc 0.858 | auc 0.915 | lr 2.0e-04\n",
      "Epoch 003 | train 0.3065 | val 0.3079 | acc 0.859 | auc 0.916 | lr 2.0e-04\n",
      "Epoch 004 | train 0.3051 | val 0.3073 | acc 0.859 | auc 0.915 | lr 2.0e-04\n",
      "Epoch 005 | train 0.3056 | val 0.3070 | acc 0.858 | auc 0.916 | lr 2.0e-04\n",
      "Epoch 006 | train 0.3057 | val 0.3070 | acc 0.858 | auc 0.916 | lr 2.0e-04\n",
      "Epoch 007 | train 0.3036 | val 0.3088 | acc 0.858 | auc 0.915 | lr 2.0e-04\n",
      "Epoch 008 | train 0.3053 | val 0.3076 | acc 0.856 | auc 0.915 | lr 2.0e-04\n",
      "Epoch 009 | train 0.3055 | val 0.3073 | acc 0.858 | auc 0.915 | lr 2.0e-05\n",
      "Epoch 010 | train 0.3038 | val 0.3074 | acc 0.858 | auc 0.915 | lr 2.0e-05\n",
      "Epoch 011 | train 0.3031 | val 0.3074 | acc 0.858 | auc 0.915 | lr 2.0e-05\n",
      "Epoch 012 | train 0.3044 | val 0.3073 | acc 0.857 | auc 0.915 | lr 2.0e-05\n",
      "Epoch 013 | train 0.3042 | val 0.3074 | acc 0.858 | auc 0.915 | lr 1.0e-05\n",
      "Epoch 014 | train 0.3028 | val 0.3074 | acc 0.858 | auc 0.915 | lr 1.0e-05\n",
      "Epoch 015 | train 0.3037 | val 0.3074 | acc 0.858 | auc 0.915 | lr 1.0e-05\n",
      "Early stopping at epoch 15 (best val loss: 0.3070)\n",
      "Loaded best model (val loss = 0.3070 )\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score\n",
    "import numpy as np\n",
    "\n",
    "#I had problems using mps so switched to cpu. \n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "# Loss, Optimizer, Scheduler \n",
    "criterion = nn.BCEWithLogitsLoss() #binary cross entropy\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=2e-4, weight_decay=1e-4)\n",
    "\n",
    "#if a metric (validation loss below) doesn't improve after 3 epochs, reduce learning rate\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, mode=\"min\", factor=0.1, patience=3, min_lr=1e-5\n",
    ")\n",
    "\n",
    "# One training epoch\n",
    "def train_one_epoch(model, loader, optimizer, criterion, device):\n",
    "    model.train() #in PyTorch there is train and eval modes \n",
    "    total_loss, total_n = 0.0, 0\n",
    "    for x_cat, x_num, yb in loader:\n",
    "        #PyTorch tensors must live on the same device as the model to interact\n",
    "        #non_blocking isn't really relevant unless using GPU\n",
    "        x_cat = x_cat.to(device, non_blocking=True)\n",
    "        x_num = x_num.to(device, non_blocking=True)\n",
    "        yb    = yb.to(device, non_blocking=True) #shape (batch_size, 1)\n",
    "\n",
    "        logits = model(x_cat, x_num) #calls the forward function of the model           \n",
    "        loss = criterion(logits, yb) #computing loss\n",
    "\n",
    "        #clears old gradients from previous step (PyTorch accumulates gradients)\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        #PyTorch computes gradient of loss w respect to each parameter by applying\n",
    "        #the chain rule through the computation graph\n",
    "        #every parameter at the end has a .grad tensor attached\n",
    "        loss.backward()\n",
    "        #rescales the gradients if their norm exceeds 1.0 preventing exploding gradients\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        #optimizer updates the parameters using the stored .grad rules\n",
    "        #for adamw, this means it does adaptive learning rate calculation + weight decay\n",
    "        # + correction before applying the update\n",
    "        optimizer.step()\n",
    "\n",
    "        batch_size = yb.size(0) #gives number of rows \n",
    "        total_loss += loss.item() * batch_size #sum of losses per batch\n",
    "        total_n += batch_size\n",
    "    return total_loss / total_n #epoch wide mean loss \n",
    "\n",
    "# Validation pass \n",
    "@torch.no_grad()\n",
    "def evaluate(model, loader, criterion, device):\n",
    "    model.eval() #put the model in eval mode \n",
    "    total_loss, total_n = 0.0, 0\n",
    "    all_probs, all_targets = [], []\n",
    "\n",
    "    for x_cat, x_num, yb in loader:\n",
    "        x_cat = x_cat.to(device, non_blocking=True)\n",
    "        x_num = x_num.to(device, non_blocking=True)\n",
    "        yb    = yb.to(device, non_blocking=True)\n",
    "\n",
    "        logits = model(x_cat, x_num)\n",
    "        loss = criterion(logits, yb)\n",
    "\n",
    "        bs = yb.size(0)\n",
    "        total_loss += loss.item() * bs\n",
    "        total_n += bs\n",
    "\n",
    "        #converts to probabilities, moves off gpu if using it, and converts to numpy array\n",
    "        #for scikit-learn metrics \n",
    "        probs = torch.sigmoid(logits).squeeze(1).cpu().numpy()\n",
    "        targets = yb.squeeze(1).cpu().numpy() #makes it (batch_size,)\n",
    "        all_probs.append(probs)\n",
    "        all_targets.append(targets)\n",
    "\n",
    "    import numpy as np\n",
    "    all_probs   = np.concatenate(all_probs, axis=0).reshape(-1)     # 1-D\n",
    "    all_targets = np.concatenate(all_targets, axis=0).reshape(-1)   # 1-D\n",
    "\n",
    "    # (optional) mask out any non-finite values, just in case\n",
    "    m = np.isfinite(all_probs) & np.isfinite(all_targets)\n",
    "    all_probs, all_targets = all_probs[m], all_targets[m]\n",
    "\n",
    "    # preds as integers (0/1)\n",
    "    preds = (all_probs >= 0.5).astype(np.int32)\n",
    "\n",
    "    from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "    acc = accuracy_score(all_targets, preds)\n",
    "\n",
    "    # auc only if both classes present\n",
    "    if np.unique(all_targets).size > 1:\n",
    "        auc = roc_auc_score(all_targets, all_probs)\n",
    "    else:\n",
    "        auc = float(\"nan\")\n",
    "\n",
    "    return (total_loss / total_n), acc, auc\n",
    "\n",
    "# Training loop with early stopping \n",
    "EPOCHS    = 200            \n",
    "PATIENCE  = 10             \n",
    "MIN_DELTA = 1e-4        \n",
    "\n",
    "best_val = float(\"inf\")\n",
    "patience_ctr = 0\n",
    "best_state = None\n",
    "\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    tr_loss = train_one_epoch(model, train_loader, optimizer, criterion, device)\n",
    "    va_loss, va_acc, va_auc = evaluate(model, val_loader, criterion, device)\n",
    "\n",
    "    #step scheduler on val loss\n",
    "    scheduler.step(va_loss)\n",
    "\n",
    "    print(f\"Epoch {epoch:03d} | \"\n",
    "          f\"train {tr_loss:.4f} | val {va_loss:.4f} | acc {va_acc:.3f} | auc {va_auc:.3f} | \"\n",
    "          f\"lr {optimizer.param_groups[0]['lr']:.1e}\")\n",
    "\n",
    "    # Early stopping\n",
    "    if va_loss < best_val - MIN_DELTA:\n",
    "        best_val = va_loss\n",
    "        best_state = {k: v.detach().cpu().clone() for k, v in model.state_dict().items()}\n",
    "        patience_ctr = 0\n",
    "    else:\n",
    "        patience_ctr += 1\n",
    "        if patience_ctr >= PATIENCE:\n",
    "            print(f\"Early stopping at epoch {epoch} (best val loss: {best_val:.4f})\")\n",
    "            break\n",
    "\n",
    "# restoring best weights \n",
    "if best_state is not None:\n",
    "    model.load_state_dict(best_state)\n",
    "    model.to(device)\n",
    "    print(\"Loaded best model (val loss =\", f\"{best_val:.4f}\", \")\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39608f36-b6bd-41bc-b9ac-71052b74b7c0",
   "metadata": {},
   "source": [
    "## XGBoost Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "55f3b3a5-596b-4956-96b4-09e7414d6598",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC: 0.929  ACC: 0.871  Best trees: 456\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "from xgboost.callback import EarlyStopping\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score\n",
    "\n",
    "X_train_xgb = X_train.copy()\n",
    "X_val_xgb = X_val.copy()\n",
    "\n",
    "for categorical in cat_cols:\n",
    "    X_train_xgb[categorical] = X_train_xgb[categorical].astype(\"category\")\n",
    "    #we will align validation categories to training categories to prevent unseen \n",
    "    X_val_xgb[categorical] = pd.Categorical(X_val_xgb[categorical], categories=X_train_xgb[categorical].cat.categories)\n",
    "\n",
    "#Building DMatrices with native categorical support\n",
    "dtrain = xgb.DMatrix(X_train_xgb, label=y_train, enable_categorical=True)\n",
    "dval   = xgb.DMatrix(X_val_xgb,   label=y_val,   enable_categorical=True)\n",
    "\n",
    "#Params (solid baseline)\n",
    "params = {\n",
    "    \"objective\": \"binary:logistic\",\n",
    "    \"eval_metric\": \"logloss\",    \n",
    "    \"max_depth\": 6,\n",
    "    \"eta\": 0.03,              \n",
    "    \"subsample\": 0.8,\n",
    "    \"colsample_bytree\": 0.8,\n",
    "    \"lambda\": 1.0,            \n",
    "    \"alpha\": 0.0,              \n",
    "    \"tree_method\": \"hist\",\n",
    "    \"verbosity\": 0,\n",
    "    \"seed\": 42,\n",
    "}\n",
    "\n",
    "#Train with early stopping on validation\n",
    "evals = [(dval, \"val\")]\n",
    "bst = xgb.train(\n",
    "    params=params,\n",
    "    dtrain=dtrain,\n",
    "    num_boost_round=2000,\n",
    "    evals=evals,\n",
    "    early_stopping_rounds=50,   # stops when val metric doesn't improve\n",
    "    verbose_eval=False,\n",
    ")\n",
    "\n",
    "#Evaluate\n",
    "probs = bst.predict(dval)   # probabilities for positive class\n",
    "preds = (probs >= 0.5).astype(int)\n",
    "\n",
    "auc = roc_auc_score(y_val, probs)\n",
    "acc = accuracy_score(y_val, preds)\n",
    "\n",
    "print(f\"AUC: {auc:.3f}  ACC: {acc:.3f}  Best trees: {bst.best_iteration}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a357440-177b-4008-b55e-b84d4165e7c3",
   "metadata": {},
   "source": [
    "# Reflection\n",
    "\n",
    "This project went really well in terms of building a full end-to-end pipeline. I was able to set up preprocessing, a custom Dataset and DataLoader, the model, the loss and optimizer, a training loop with early stopping and a learning rate scheduler, and then track metrics properly. Along the way, I developed a much clearer mental model of key PyTorch concepts like logits vs. probabilities, why BCEWithLogitsLoss is preferred, the role of .train() and .eval(), device transfers, batch sizing, and how to aggregate batch metrics. I also handled categorical embeddings correctly, with per-column vocabularies and UNK handling, and built some debugging intuition by catching NaNs during initialization and tracing them back to Apple GPU issues. On top of that, I established a strong baseline by running XGBoost and comparing it to the neural network.\n",
    "\n",
    "What didnt go so smoothly was that the validation loss plateaued at around 0.307, regardless of scheduler tweaks or learning rate changes. This turned out not to be a problem with optimization but more a sign of architectural capacity limits in the MLP. Using Apples MPS backend also caused NaN issues, which forced me back to CPU training for stability. I also leaned a little too quickly on Optuna, when in hindsight the flat validation curve indicated that I should have tried architectural changes or regularization adjustments first.\n",
    "\n",
    "The main lessons Im taking away are that preprocessing must be done carefully to avoid leakage, embeddings help compress sparse categorical data into useful representations, and schedulers should always monitor validation performance, not just training. I also learned its often better to slightly over-parameterize a network and regularize it than to under-parameterize and cap performance. XGBoost slightly outperformed the neural network here, which is expected in tabular settings because trees are so good at capturing threshold-like interactions that MLPs miss without more advanced architectures.\n",
    "\n",
    "If I were to run this again, I would try a larger MLP with batch normalization and lighter dropout, experiment with more dynamic learning rate schedules like OneCycle, and consider using pos_weight in the loss if the classes are imbalanced. Id also look at engineered interaction features to see if the neural net can learn from what boosts XGBoost. For a bigger leap, Id try transformer-style tabular models like FT-Transformer and compare them against XGBoost on a larger dataset. Overall, the biggest win is that I now have a clean, reusable training loop, a dataset pipeline, a hyperparameter tuning scaffold, and a strong baseline comparison  all of which I can carry into my next project.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84ce93de-2c0b-4d9c-b420-6476c35a2b9e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
